{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd35d3c9",
   "metadata": {},
   "source": [
    "### Instalação das dependências\n",
    "Como este notebook está dentro do projeto do TCC é necessário instalar somente o hyperas. Caso não tenha as outras dependências basta rodar o arquivo de **requirements.txt** presente na raiz da aplicação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd23f2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperas in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: entrypoints in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from hyperas) (0.3)\n",
      "Requirement already satisfied: keras in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from hyperas) (2.4.3)\n",
      "Requirement already satisfied: hyperopt in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from hyperas) (0.2.5)\n",
      "Requirement already satisfied: nbconvert in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from hyperas) (6.0.7)\n",
      "Requirement already satisfied: nbformat in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from hyperas) (5.1.3)\n",
      "Requirement already satisfied: jupyter in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from hyperas) (1.0.0)\n",
      "Requirement already satisfied: six in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from hyperopt->hyperas) (1.15.0)\n",
      "Requirement already satisfied: future in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from hyperopt->hyperas) (0.18.2)\n",
      "Requirement already satisfied: scipy in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from hyperopt->hyperas) (1.5.4)\n",
      "Requirement already satisfied: cloudpickle in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from hyperopt->hyperas) (1.6.0)\n",
      "Requirement already satisfied: tqdm in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from hyperopt->hyperas) (4.60.0)\n",
      "Requirement already satisfied: networkx>=2.2 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from hyperopt->hyperas) (2.5.1)\n",
      "Requirement already satisfied: numpy in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from hyperopt->hyperas) (1.19.5)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from networkx>=2.2->hyperopt->hyperas) (4.4.2)\n",
      "Requirement already satisfied: qtconsole in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from jupyter->hyperas) (5.1.1)\n",
      "Requirement already satisfied: ipykernel in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from jupyter->hyperas) (5.5.5)\n",
      "Requirement already satisfied: jupyter-console in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from jupyter->hyperas) (6.4.0)\n",
      "Requirement already satisfied: notebook in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from jupyter->hyperas) (6.4.0)\n",
      "Requirement already satisfied: ipywidgets in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from jupyter->hyperas) (7.6.3)\n",
      "Requirement already satisfied: jupyter-client in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from ipykernel->jupyter->hyperas) (6.1.12)\n",
      "Requirement already satisfied: ipython>=5.0.0 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from ipykernel->jupyter->hyperas) (7.16.1)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from ipykernel->jupyter->hyperas) (4.3.3)\n",
      "Requirement already satisfied: tornado>=4.2 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from ipykernel->jupyter->hyperas) (6.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperas) (3.0.19)\n",
      "Requirement already satisfied: setuptools>=18.5 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperas) (54.2.0)\n",
      "Requirement already satisfied: backcall in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperas) (0.2.0)\n",
      "Requirement already satisfied: pygments in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperas) (2.9.0)\n",
      "Requirement already satisfied: jedi>=0.10 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperas) (0.18.0)\n",
      "Requirement already satisfied: pickleshare in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperas) (0.7.5)\n",
      "Requirement already satisfied: colorama in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperas) (0.4.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter->hyperas) (0.8.2)\n",
      "Requirement already satisfied: wcwidth in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->hyperas) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from traitlets>=4.1.0->ipykernel->jupyter->hyperas) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from ipywidgets->jupyter->hyperas) (1.0.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from ipywidgets->jupyter->hyperas) (3.5.1)\n",
      "Requirement already satisfied: jupyter-core in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from nbformat->hyperas) (4.7.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from nbformat->hyperas) (3.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->hyperas) (0.18.0)\n",
      "Requirement already satisfied: importlib-metadata in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->hyperas) (3.10.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->hyperas) (21.2.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from notebook->jupyter->hyperas) (1.7.1)\n",
      "Requirement already satisfied: pyzmq>=17 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from notebook->jupyter->hyperas) (22.1.0)\n",
      "Requirement already satisfied: jinja2 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from notebook->jupyter->hyperas) (3.0.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from notebook->jupyter->hyperas) (0.10.1)\n",
      "Requirement already satisfied: argon2-cffi in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from notebook->jupyter->hyperas) (20.1.0)\n",
      "Requirement already satisfied: prometheus-client in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from notebook->jupyter->hyperas) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from jupyter-client->ipykernel->jupyter->hyperas) (2.8.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from jupyter-core->nbformat->hyperas) (301)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from terminado>=0.8.3->notebook->jupyter->hyperas) (1.1.3)\n",
      "Requirement already satisfied: cffi>=1.0.0 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from argon2-cffi->notebook->jupyter->hyperas) (1.14.6)\n",
      "Requirement already satisfied: pycparser in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter->hyperas) (2.20)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat->hyperas) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat->hyperas) (3.7.4.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from jinja2->notebook->jupyter->hyperas) (2.0.1)\n",
      "Requirement already satisfied: h5py in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from keras->hyperas) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from keras->hyperas) (5.4.1)\n",
      "Requirement already satisfied: defusedxml in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from nbconvert->hyperas) (0.7.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from nbconvert->hyperas) (0.8.4)\n",
      "Requirement already satisfied: bleach in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from nbconvert->hyperas) (3.3.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from nbconvert->hyperas) (0.1.2)\n",
      "Requirement already satisfied: testpath in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from nbconvert->hyperas) (0.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from nbconvert->hyperas) (0.5.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from nbconvert->hyperas) (1.4.3)\n",
      "Requirement already satisfied: nest-asyncio in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->hyperas) (1.5.1)\n",
      "Requirement already satisfied: async-generator in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->hyperas) (1.10)\n",
      "Requirement already satisfied: webencodings in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
      "Requirement already satisfied: packaging in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from bleach->nbconvert->hyperas) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from packaging->bleach->nbconvert->hyperas) (2.4.7)\n",
      "Requirement already satisfied: qtpy in d:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\lib\\site-packages (from qtconsole->jupyter->hyperas) (1.9.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the 'd:\\documentos\\harã stuffs\\faculdade\\tcc\\codes\\tcc-rede-neural-siamesa\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2ca93",
   "metadata": {},
   "source": [
    "### Importações das dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679fd163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Input, Embedding, LSTM, Lambda, Conv1D, Dense, Dropout, Activation, Bidirectional\n",
    "from tensorflow.python.keras import backend as k\n",
    "from tensorflow.python.keras.layers import Lambda, Reshape, dot\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4bddc",
   "metadata": {},
   "source": [
    "### Definição dos paths padrões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dbf61d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_FILES_INDEX_VECTORS_PATH = os.path.join(os.path.dirname(os.path.abspath(\"\")), \"data\", \"processed/index_vectors\")\n",
    "DATA_FILES_EMBEDDING_MATRICES_PATH = os.path.join(os.path.dirname(os.path.abspath(\"\")), \"data\", \"processed/embedding_matrices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1d05f",
   "metadata": {},
   "source": [
    "### Carregando os vetores de índices\n",
    "São os vetores que são alimentados na entrada da rede neural siamesa, onde cada valor representa **o índice correspondente na matriz de incoporação**. Ambos (matriz de incoporação e vetor de índice) são armazenados na aplicação após ser aplicado o processo de estruturação de dados. Para cada word embedding utilizado foi criado um arquivo que armazena os vetores de índices para os datasets: cru (raw), sem stopwords (sw) e sem stopwords e com lematização (sw_lemmatization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31042930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    ### FUNÇÕES INTERNAS ###\n",
    "    \n",
    "    def load_index_vector_dataframe(filename):\n",
    "        dataframe = pd.read_csv(filename)\n",
    "\n",
    "        for q in ['phrase1', 'phrase2']:\n",
    "            dataframe[q + '_n'] = dataframe[q].apply(lambda x: [int(i) for i in x.replace('[', '').replace(']', '').split(',')])\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    def split_data_train(train_dataframe):\n",
    "        x_phrases = train_dataframe[['phrase1_n', 'phrase2_n']]\n",
    "\n",
    "        train_dataframe.label = pd.Categorical(train_dataframe.label)\n",
    "        train_dataframe['label'] = train_dataframe.label.cat.codes\n",
    "        y_labels = train_dataframe['label']\n",
    "\n",
    "        return x_phrases, y_labels\n",
    "    \n",
    "    def split_and_zero_padding(dataframe, max_seq_length):\n",
    "        # Split to dicts\n",
    "        side_phrases = {'left': dataframe['phrase1_n'], 'right': dataframe['phrase2_n']}\n",
    "        dataset = None\n",
    "\n",
    "        # Zero padding\n",
    "        for dataset, side in itertools.product([side_phrases], ['left', 'right']):\n",
    "            dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    ### FUNÇÕES INTERNAS ###\n",
    "    \n",
    "    \n",
    "    ### ARQUIVOS VETOR INDICES ###\n",
    "\n",
    "    DATA_FILES_INDEX_VECTORS_PATH = os.path.join(os.path.dirname(os.path.abspath(\"\")), \"data\", \"processed/index_vectors\")\n",
    "    \n",
    "    # Word2vec Google News\n",
    "    raw_w2v_GN = os.path.join(DATA_FILES_INDEX_VECTORS_PATH, \"training-raw-w2v_GN.csv\")\n",
    "    sw_w2v_GN = os.path.join(DATA_FILES_INDEX_VECTORS_PATH, \"training-sw-w2v_GN.csv\")\n",
    "    sw_lemma_w2v_GN = os.path.join(DATA_FILES_INDEX_VECTORS_PATH, \"training-sw-lemmatization-w2v_GN.csv\")\n",
    "    \n",
    "    # Word2vec Wikipedia\n",
    "    \n",
    "    # Glove Wikipedia + Gigaword\n",
    "    \n",
    "    # Glove Common Crawl\n",
    "    \n",
    "    ### ARQUIVOS WORD EMBEDDING ###\n",
    "    \n",
    "    \n",
    "    ### TIPOS DE MAX_SEQ_LENGTH ###\n",
    "\n",
    "    MAX_SEQ_LENGTH_RAW = 17\n",
    "    MAX_SEQ_LENGTH_SW = 9\n",
    "    MAX_SEQ_LENGTH_SW_LEMMA = 9\n",
    "\n",
    "    max_seq_length = MAX_SEQ_LENGTH_RAW\n",
    "\n",
    "    ### TIPOS DE MAX_SEQ_LENGTH ###\n",
    "    \n",
    "    \n",
    "    # Carregamento do vetor de índices através do pandas    \n",
    "    train_dataframe = load_index_vector_dataframe(raw_w2v_GN)\n",
    "    \n",
    "    # Divisão do dataset de entrada da rede neural em TREINAMENTO e TESTE (PREDIÇÃO)\n",
    "    x_phrases, y_labels = split_data_train(train_dataframe)\n",
    "    \n",
    "    # Divisão 90/10 de treinamento e teste\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_phrases, y_labels, test_size=0.1, random_state=0, stratify=train_dataframe['label'])\n",
    "    \n",
    "    # Divisão dos dados de TREINAMENTO de entrada entre a parte esquerda e direita das subredes e zero padding à esquerda dos dados de TREINAMENTO (retorno em ndarray)\n",
    "    x_train = split_and_zero_padding(x_train, max_seq_length)\n",
    "\n",
    "    # Divisão dos dados de TESTE (predição) de entrada entre a parte esquerda e direita das subredes e zero padding à esquerda dos dados de TESTE (predição) (retorno em ndarray)\n",
    "    x_test = split_and_zero_padding(x_test, max_seq_length)\n",
    "\n",
    "    # Conversão para numpy das labels\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5804b1",
   "metadata": {},
   "source": [
    "### Criação do modelo da rede siamesa\n",
    "Criação do modelo da rede siamesa utilizando como subredes internas a arquitetura *LSTM (Long Short Term Memory)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd89b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    ### FUNÇÕES INTERNAS ###\n",
    "\n",
    "    # Distância de Manhattan\n",
    "    def define_manhattan_model(shared_model, max_seq_length):\n",
    "        def calculate_manhattan_distance(left_output, right_output):          \n",
    "            def __exponent_neg_manhattan_distance(left, right):\n",
    "                return k.exp(-k.sum(k.abs(left - right), axis=1, keepdims=True))\n",
    "\n",
    "            manhattan_distance = Lambda(\n",
    "                function=lambda x: __exponent_neg_manhattan_distance(x[0], x[1]),\n",
    "                output_shape=lambda x: (x[0][0], 1)\n",
    "            )([left_output, right_output])\n",
    "\n",
    "            return manhattan_distance\n",
    "\n",
    "        # The visible layer\n",
    "        left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "        right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "        # Pack it all up into a Manhattan Distance model\n",
    "        malstm_distance = calculate_manhattan_distance(shared_model(left_input), shared_model(right_input))\n",
    "        manhattan_model = Model(inputs=[left_input, right_input], outputs=[malstm_distance])\n",
    "\n",
    "        return manhattan_model\n",
    "\n",
    "    # Similaridade de Cossenos\n",
    "    def define_cosine_model(shared_model, max_seq_length):\n",
    "        def calculate_cosine_distance(left_output, right_output):\n",
    "            cos_distance = dot([left_output, right_output], axes=1, normalize=True)\n",
    "            cos_distance = Reshape((1,))(cos_distance)\n",
    "            cos_similarity = Lambda(lambda x: 1 - x)(cos_distance)\n",
    "\n",
    "            return cos_similarity\n",
    "\n",
    "        left_input = Input(shape=(max_seq_length,))\n",
    "        right_input = Input(shape=(max_seq_length,))\n",
    "\n",
    "        cosine_distance = calculate_cosine_distance(shared_model(left_input), shared_model(right_input))\n",
    "        cosine_model = Model(inputs=[left_input, right_input], outputs=[cosine_distance])\n",
    "\n",
    "        return cosine_model\n",
    "\n",
    "    # Distância Euclidiana\n",
    "    def define_euclidean_model(shared_model, max_seq_length):\n",
    "        def calculate_euclidean_distance(vects):\n",
    "            x, y = vects\n",
    "            sum_square = k.sum(k.square(x - y), axis=1, keepdims=True)\n",
    "\n",
    "            return k.sqrt(k.maximum(sum_square, k.epsilon()))\n",
    "\n",
    "        def dist_output_shape(shapes):\n",
    "            shape1, shape2 = shapes\n",
    "\n",
    "            return (shape1[0], 1)\n",
    "\n",
    "        left_input = Input(shape=(max_seq_length,))\n",
    "        right_input = Input(shape=(max_seq_length,))\n",
    "\n",
    "        euclidean_distance = Lambda(\n",
    "            calculate_euclidean_distance,\n",
    "            output_shape=dist_output_shape\n",
    "        )([shared_model(left_input), shared_model(right_input)])\n",
    "        euclidean_model = Model(inputs=[left_input, right_input], outputs=[euclidean_distance])\n",
    "\n",
    "        return euclidean_model\n",
    "\n",
    "    ### FUNÇÕES INTERNAS ###\n",
    "\n",
    "\n",
    "    ### TIPOS DE MAX_SEQ_LENGTH ###\n",
    "\n",
    "    MAX_SEQ_LENGTH_RAW = 17\n",
    "    MAX_SEQ_LENGTH_SW = 9\n",
    "    MAX_SEQ_LENGTH_SW_LEMMA = 9\n",
    "\n",
    "    max_seq_length = MAX_SEQ_LENGTH_RAW\n",
    "\n",
    "    ### TIPOS DE MAX_SEQ_LENGTH ###\n",
    "\n",
    "\n",
    "    ### ARQUIVOS EMBEDDINGS ###\n",
    "    \n",
    "    DATA_FILES_EMBEDDING_MATRICES_PATH = os.path.join(os.path.dirname(os.path.abspath(\"\")), \"data\", \"processed/embedding_matrices\")\n",
    "    \n",
    "    # Word2vec Google News\n",
    "    raw_w2v_GN = os.path.join(DATA_FILES_EMBEDDING_MATRICES_PATH, \"raw-w2v_GN.npy\")\n",
    "    sw_w2v_GN = os.path.join(DATA_FILES_EMBEDDING_MATRICES_PATH, \"sw-w2v_GN.npy\")\n",
    "    sw_lemma_w2v_GN = os.path.join(DATA_FILES_EMBEDDING_MATRICES_PATH, \"sw-lemmatization-w2v_GN.npy\")\n",
    "\n",
    "    # Word2vec Wikipedia\n",
    "\n",
    "    # Glove Wikipedia + Gigaword\n",
    "\n",
    "    # Glove Common Crawl\n",
    "\n",
    "    ### ARQUIVOS EMBEDDINGS ###\n",
    "\n",
    "\n",
    "    # Definição do modelo compartilhado (shared model) entre as subredes, pois são idênticas\n",
    "    embedding_dim = 300\n",
    "    embedding_matrix = np.load(raw_w2v_GN)\n",
    "\n",
    "    shared_model = Sequential()\n",
    "    shared_model.add(\n",
    "        Embedding(\n",
    "            len(embedding_matrix),\n",
    "            embedding_dim,\n",
    "            weights=[embedding_matrix],\n",
    "            input_shape=(max_seq_length,),\n",
    "            trainable=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    shared_model.add(Dropout({{uniform(0, 1)}}))\n",
    "    shared_model.add(Bidirectional(LSTM(\n",
    "        {{choice([64, 128, 256, 512])}},\n",
    "        kernel_initializer={{choice([tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='truncated_normal',seed=1), tf.keras.initializers.glorot_normal(seed=1)])}},\n",
    "        activation='softsign',\n",
    "        recurrent_activation='sigmoid',\n",
    "        dropout=0.0,\n",
    "        recurrent_dropout={{uniform(0, 1)}},\n",
    "        implementation=1\n",
    "    )))\n",
    "    shared_model.add(Activation({{choice(['relu', 'selu', 'elu'])}}))\n",
    "    shared_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Define o lado esquerdo e direito das subredes a partir da shared_model. Também define a medida/função de similaridade usada na camada de merge na saída da rede\n",
    "    model = define_manhattan_model(shared_model, max_seq_length)\n",
    "\n",
    "    # Compilação do modelo da rede siamesa\n",
    "    model.compile(\n",
    "        loss={{choice([tf.keras.losses.MeanSquaredError(), tfa.losses.ContrastiveLoss(), tfa.losses.TripletSemiHardLoss(), tfa.losses.TripletHardLoss(), tf.keras.losses.BinaryCrossentropy()])}},\n",
    "        optimizer={{choice([tf.keras.optimizers.Adadelta(learning_rate=0.1, rho=0.95, epsilon=1e-07, name='Adadelta', clipnorm=1.5),\n",
    "                            tf.keras.optimizers.Adamax(learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"),\n",
    "                            tf.keras.optimizers.Adagrad(learning_rate=0.1,  initial_accumulator_value=0.1,epsilon=1e-07, name='Adagrad', clipnorm=1.5),\n",
    "                            tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.0, nesterov=False, name='SGD'),\n",
    "                            tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "                            tf.keras.optimizers.RMSprop(learning_rate=0.1)])}},\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Execução do treinamento da rede\n",
    "    training_history = model.fit(\n",
    "        [x_train['left'], x_train['right']],\n",
    "        y_train,\n",
    "        batch_size={{choice([16, 32, 64, 128, 256])}},\n",
    "        epochs={{choice([10, 20])}},\n",
    "        verbose=2,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    return {'loss': -(np.amax(training_history.history['val_accuracy'])), 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c52199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partes substituidas para teste\n",
    "# {{choice([50, 100, 150, 200])}}\n",
    "# optimizer= tf.keras.optimizers.Adam(learning_rate={{choice([0.001, 0.01, 0.1])}}),\n",
    "# batch_size={{choice([8, 16, 32, 64, 128, 256])}},\n",
    "#epochs=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413f547",
   "metadata": {},
   "source": [
    "### Execução do Hyperas\n",
    "Execução das funções com os códigos básicos do Hyperas retornando o melhor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ff98a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import itertools\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow_addons as tfa\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras.models import Model, Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras.layers import Input, Embedding, LSTM, Lambda, Conv1D, Dense, Dropout, Activation, Bidirectional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras import backend as k\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras.layers import Lambda, Reshape, dot\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'LSTM': hp.choice('LSTM', [64, 128, 256, 512]),\n",
      "        'kernel_initializer': hp.choice('kernel_initializer', [tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='truncated_normal',seed=1), tf.keras.initializers.glorot_normal(seed=1)]),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'Activation': hp.choice('Activation', ['relu', 'selu', 'elu']),\n",
      "        'loss': hp.choice('loss', [tf.keras.losses.MeanSquaredError(), tfa.losses.ContrastiveLoss(), tfa.losses.TripletSemiHardLoss(), tfa.losses.TripletHardLoss(), tf.keras.losses.BinaryCrossentropy()]),\n",
      "        'optimizer': hp.choice('optimizer', [tf.keras.optimizers.Adadelta(learning_rate=0.1, rho=0.95, epsilon=1e-07, name='Adadelta', clipnorm=1.5),\n",
      "                            tf.keras.optimizers.Adamax(learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"),\n",
      "                            tf.keras.optimizers.Adagrad(learning_rate=0.1,  initial_accumulator_value=0.1,epsilon=1e-07, name='Adagrad', clipnorm=1.5),\n",
      "                            tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.0, nesterov=False, name='SGD'),\n",
      "                            tf.keras.optimizers.Adam(learning_rate=0.1),\n",
      "                            tf.keras.optimizers.RMSprop(learning_rate=0.1)]),\n",
      "        'batch_size': hp.choice('batch_size', [16, 32, 64, 128, 256]),\n",
      "        'epochs': hp.choice('epochs', [10, 20]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: ### FUNÇÕES INTERNAS ###\n",
      "   3: \n",
      "   4: def load_index_vector_dataframe(filename):\n",
      "   5:     dataframe = pd.read_csv(filename)\n",
      "   6: \n",
      "   7:     for q in ['phrase1', 'phrase2']:\n",
      "   8:         dataframe[q + '_n'] = dataframe[q].apply(lambda x: [int(i) for i in x.replace('[', '').replace(']', '').split(',')])\n",
      "   9: \n",
      "  10:     return dataframe\n",
      "  11: \n",
      "  12: def split_data_train(train_dataframe):\n",
      "  13:     x_phrases = train_dataframe[['phrase1_n', 'phrase2_n']]\n",
      "  14: \n",
      "  15:     train_dataframe.label = pd.Categorical(train_dataframe.label)\n",
      "  16:     train_dataframe['label'] = train_dataframe.label.cat.codes\n",
      "  17:     y_labels = train_dataframe['label']\n",
      "  18: \n",
      "  19:     return x_phrases, y_labels\n",
      "  20: \n",
      "  21: def split_and_zero_padding(dataframe, max_seq_length):\n",
      "  22:     # Split to dicts\n",
      "  23:     side_phrases = {'left': dataframe['phrase1_n'], 'right': dataframe['phrase2_n']}\n",
      "  24:     dataset = None\n",
      "  25: \n",
      "  26:     # Zero padding\n",
      "  27:     for dataset, side in itertools.product([side_phrases], ['left', 'right']):\n",
      "  28:         dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n",
      "  29: \n",
      "  30:     return dataset\n",
      "  31: \n",
      "  32: ### FUNÇÕES INTERNAS ###\n",
      "  33: \n",
      "  34: \n",
      "  35: ### ARQUIVOS VETOR INDICES ###\n",
      "  36: \n",
      "  37: DATA_FILES_INDEX_VECTORS_PATH = os.path.join(os.path.dirname(os.path.abspath(\"\")), \"data\", \"processed/index_vectors\")\n",
      "  38: \n",
      "  39: # Word2vec Google News\n",
      "  40: raw_w2v_GN = os.path.join(DATA_FILES_INDEX_VECTORS_PATH, \"training-raw-w2v_GN.csv\")\n",
      "  41: sw_w2v_GN = os.path.join(DATA_FILES_INDEX_VECTORS_PATH, \"training-sw-w2v_GN.csv\")\n",
      "  42: sw_lemma_w2v_GN = os.path.join(DATA_FILES_INDEX_VECTORS_PATH, \"training-sw-lemmatization-w2v_GN.csv\")\n",
      "  43: \n",
      "  44: # Word2vec Wikipedia\n",
      "  45: \n",
      "  46: # Glove Wikipedia + Gigaword\n",
      "  47: \n",
      "  48: # Glove Common Crawl\n",
      "  49: \n",
      "  50: ### ARQUIVOS WORD EMBEDDING ###\n",
      "  51: \n",
      "  52: \n",
      "  53: ### TIPOS DE MAX_SEQ_LENGTH ###\n",
      "  54: \n",
      "  55: MAX_SEQ_LENGTH_RAW = 17\n",
      "  56: MAX_SEQ_LENGTH_SW = 9\n",
      "  57: MAX_SEQ_LENGTH_SW_LEMMA = 9\n",
      "  58: \n",
      "  59: max_seq_length = MAX_SEQ_LENGTH_RAW\n",
      "  60: \n",
      "  61: ### TIPOS DE MAX_SEQ_LENGTH ###\n",
      "  62: \n",
      "  63: \n",
      "  64: # Carregamento do vetor de índices através do pandas    \n",
      "  65: train_dataframe = load_index_vector_dataframe(raw_w2v_GN)\n",
      "  66: \n",
      "  67: # Divisão do dataset de entrada da rede neural em TREINAMENTO e TESTE (PREDIÇÃO)\n",
      "  68: x_phrases, y_labels = split_data_train(train_dataframe)\n",
      "  69: \n",
      "  70: # Divisão 90/10 de treinamento e teste\n",
      "  71: x_train, x_test, y_train, y_test = train_test_split(x_phrases, y_labels, test_size=0.1, random_state=0, stratify=train_dataframe['label'])\n",
      "  72: \n",
      "  73: # Divisão dos dados de TREINAMENTO de entrada entre a parte esquerda e direita das subredes e zero padding à esquerda dos dados de TREINAMENTO (retorno em ndarray)\n",
      "  74: x_train = split_and_zero_padding(x_train, max_seq_length)\n",
      "  75: \n",
      "  76: # Divisão dos dados de TESTE (predição) de entrada entre a parte esquerda e direita das subredes e zero padding à esquerda dos dados de TESTE (predição) (retorno em ndarray)\n",
      "  77: x_test = split_and_zero_padding(x_test, max_seq_length)\n",
      "  78: \n",
      "  79: # Conversão para numpy das labels\n",
      "  80: y_train = y_train.values\n",
      "  81: y_test = y_test.values\n",
      "  82: \n",
      "  83: \n",
      "  84: \n",
      "  85: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \n",
      "   4:     ### FUNÇÕES INTERNAS ###\n",
      "   5: \n",
      "   6:     # Distância de Manhattan\n",
      "   7:     def define_manhattan_model(shared_model, max_seq_length):\n",
      "   8:         def calculate_manhattan_distance(left_output, right_output):          \n",
      "   9:             def __exponent_neg_manhattan_distance(left, right):\n",
      "  10:                 return k.exp(-k.sum(k.abs(left - right), axis=1, keepdims=True))\n",
      "  11: \n",
      "  12:             manhattan_distance = Lambda(\n",
      "  13:                 function=lambda x: __exponent_neg_manhattan_distance(x[0], x[1]),\n",
      "  14:                 output_shape=lambda x: (x[0][0], 1)\n",
      "  15:             )([left_output, right_output])\n",
      "  16: \n",
      "  17:             return manhattan_distance\n",
      "  18: \n",
      "  19:         # The visible layer\n",
      "  20:         left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
      "  21:         right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
      "  22: \n",
      "  23:         # Pack it all up into a Manhattan Distance model\n",
      "  24:         malstm_distance = calculate_manhattan_distance(shared_model(left_input), shared_model(right_input))\n",
      "  25:         manhattan_model = Model(inputs=[left_input, right_input], outputs=[malstm_distance])\n",
      "  26: \n",
      "  27:         return manhattan_model\n",
      "  28: \n",
      "  29:     # Similaridade de Cossenos\n",
      "  30:     def define_cosine_model(shared_model, max_seq_length):\n",
      "  31:         def calculate_cosine_distance(left_output, right_output):\n",
      "  32:             cos_distance = dot([left_output, right_output], axes=1, normalize=True)\n",
      "  33:             cos_distance = Reshape((1,))(cos_distance)\n",
      "  34:             cos_similarity = Lambda(lambda x: 1 - x)(cos_distance)\n",
      "  35: \n",
      "  36:             return cos_similarity\n",
      "  37: \n",
      "  38:         left_input = Input(shape=(max_seq_length,))\n",
      "  39:         right_input = Input(shape=(max_seq_length,))\n",
      "  40: \n",
      "  41:         cosine_distance = calculate_cosine_distance(shared_model(left_input), shared_model(right_input))\n",
      "  42:         cosine_model = Model(inputs=[left_input, right_input], outputs=[cosine_distance])\n",
      "  43: \n",
      "  44:         return cosine_model\n",
      "  45: \n",
      "  46:     # Distância Euclidiana\n",
      "  47:     def define_euclidean_model(shared_model, max_seq_length):\n",
      "  48:         def calculate_euclidean_distance(vects):\n",
      "  49:             x, y = vects\n",
      "  50:             sum_square = k.sum(k.square(x - y), axis=1, keepdims=True)\n",
      "  51: \n",
      "  52:             return k.sqrt(k.maximum(sum_square, k.epsilon()))\n",
      "  53: \n",
      "  54:         def dist_output_shape(shapes):\n",
      "  55:             shape1, shape2 = shapes\n",
      "  56: \n",
      "  57:             return (shape1[0], 1)\n",
      "  58: \n",
      "  59:         left_input = Input(shape=(max_seq_length,))\n",
      "  60:         right_input = Input(shape=(max_seq_length,))\n",
      "  61: \n",
      "  62:         euclidean_distance = Lambda(\n",
      "  63:             calculate_euclidean_distance,\n",
      "  64:             output_shape=dist_output_shape\n",
      "  65:         )([shared_model(left_input), shared_model(right_input)])\n",
      "  66:         euclidean_model = Model(inputs=[left_input, right_input], outputs=[euclidean_distance])\n",
      "  67: \n",
      "  68:         return euclidean_model\n",
      "  69: \n",
      "  70:     ### FUNÇÕES INTERNAS ###\n",
      "  71: \n",
      "  72: \n",
      "  73:     ### TIPOS DE MAX_SEQ_LENGTH ###\n",
      "  74: \n",
      "  75:     MAX_SEQ_LENGTH_RAW = 17\n",
      "  76:     MAX_SEQ_LENGTH_SW = 9\n",
      "  77:     MAX_SEQ_LENGTH_SW_LEMMA = 9\n",
      "  78: \n",
      "  79:     max_seq_length = MAX_SEQ_LENGTH_RAW\n",
      "  80: \n",
      "  81:     ### TIPOS DE MAX_SEQ_LENGTH ###\n",
      "  82: \n",
      "  83: \n",
      "  84:     ### ARQUIVOS EMBEDDINGS ###\n",
      "  85:     \n",
      "  86:     DATA_FILES_EMBEDDING_MATRICES_PATH = os.path.join(os.path.dirname(os.path.abspath(\"\")), \"data\", \"processed/embedding_matrices\")\n",
      "  87:     \n",
      "  88:     # Word2vec Google News\n",
      "  89:     raw_w2v_GN = os.path.join(DATA_FILES_EMBEDDING_MATRICES_PATH, \"raw-w2v_GN.npy\")\n",
      "  90:     sw_w2v_GN = os.path.join(DATA_FILES_EMBEDDING_MATRICES_PATH, \"sw-w2v_GN.npy\")\n",
      "  91:     sw_lemma_w2v_GN = os.path.join(DATA_FILES_EMBEDDING_MATRICES_PATH, \"sw-lemmatization-w2v_GN.npy\")\n",
      "  92: \n",
      "  93:     # Word2vec Wikipedia\n",
      "  94: \n",
      "  95:     # Glove Wikipedia + Gigaword\n",
      "  96: \n",
      "  97:     # Glove Common Crawl\n",
      "  98: \n",
      "  99:     ### ARQUIVOS EMBEDDINGS ###\n",
      " 100: \n",
      " 101: \n",
      " 102:     # Definição do modelo compartilhado (shared model) entre as subredes, pois são idênticas\n",
      " 103:     embedding_dim = 300\n",
      " 104:     embedding_matrix = np.load(raw_w2v_GN)\n",
      " 105: \n",
      " 106:     shared_model = Sequential()\n",
      " 107:     shared_model.add(\n",
      " 108:         Embedding(\n",
      " 109:             len(embedding_matrix),\n",
      " 110:             embedding_dim,\n",
      " 111:             weights=[embedding_matrix],\n",
      " 112:             input_shape=(max_seq_length,),\n",
      " 113:             trainable=False\n",
      " 114:         )\n",
      " 115:     )\n",
      " 116:     \n",
      " 117:     shared_model.add(Dropout(space['Dropout']))\n",
      " 118:     shared_model.add(Bidirectional(LSTM(\n",
      " 119:         space['LSTM'],\n",
      " 120:         kernel_initializer=space['kernel_initializer'],\n",
      " 121:         activation='softsign',\n",
      " 122:         recurrent_activation='sigmoid',\n",
      " 123:         dropout=0.0,\n",
      " 124:         recurrent_dropout=space['Dropout_1'],\n",
      " 125:         implementation=1\n",
      " 126:     )))\n",
      " 127:     shared_model.add(Activation(space['Activation']))\n",
      " 128:     shared_model.add(Dense(1, activation='sigmoid'))\n",
      " 129: \n",
      " 130:     # Define o lado esquerdo e direito das subredes a partir da shared_model. Também define a medida/função de similaridade usada na camada de merge na saída da rede\n",
      " 131:     model = define_manhattan_model(shared_model, max_seq_length)\n",
      " 132: \n",
      " 133:     # Compilação do modelo da rede siamesa\n",
      " 134:     model.compile(\n",
      " 135:         loss=space['loss'],\n",
      " 136:         optimizer=space['optimizer'],\n",
      " 137:         metrics=['accuracy']\n",
      " 138:     )\n",
      " 139: \n",
      " 140:     # Execução do treinamento da rede\n",
      " 141:     training_history = model.fit(\n",
      " 142:         [x_train['left'], x_train['right']],\n",
      " 143:         y_train,\n",
      " 144:         batch_size=space['batch_size'],\n",
      " 145:         epochs=space['epochs'],\n",
      " 146:         verbose=2,\n",
      " 147:         validation_split=0.2\n",
      " 148:     )\n",
      " 149:     \n",
      " 150:     return {'loss': -(np.amax(training_history.history['val_accuracy'])), 'status': STATUS_OK, 'model': model}\n",
      " 151: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20                                                                                                                                                                                \n",
      "765/765 - 41s - loss: 0.3322 - accuracy: 0.5197 - val_loss: 0.3222 - val_accuracy: 0.5324                                                                                                 \n",
      "\n",
      "Epoch 2/20                                                                                                                                                                                \n",
      "765/765 - 37s - loss: 0.3135 - accuracy: 0.5554 - val_loss: 0.3307 - val_accuracy: 0.5402                                                                                                 \n",
      "\n",
      "Epoch 3/20                                                                                                                                                                                \n",
      "765/765 - 37s - loss: 0.3110 - accuracy: 0.5613 - val_loss: 0.3279 - val_accuracy: 0.5466                                                                                                 \n",
      "\n",
      "Epoch 4/20                                                                                                                                                                                \n",
      "765/765 - 38s - loss: 0.3057 - accuracy: 0.5716 - val_loss: 0.3235 - val_accuracy: 0.5467                                                                                                 \n",
      "\n",
      "Epoch 5/20                                                                                                                                                                                \n",
      "765/765 - 37s - loss: 0.2966 - accuracy: 0.5845 - val_loss: 0.3300 - val_accuracy: 0.5462                                                                                                 \n",
      "\n",
      "Epoch 6/20                                                                                                                                                                                \n",
      "765/765 - 37s - loss: 0.2994 - accuracy: 0.5833 - val_loss: 0.3210 - val_accuracy: 0.5471                                                                                                 \n",
      "\n",
      "Epoch 7/20                                                                                                                                                                                \n",
      "765/765 - 37s - loss: 0.2925 - accuracy: 0.5937 - val_loss: 0.3204 - val_accuracy: 0.5436                                                                                                 \n",
      "\n",
      "Epoch 8/20                                                                                                                                                                                \n",
      "765/765 - 36s - loss: 0.2878 - accuracy: 0.6007 - val_loss: 0.3247 - val_accuracy: 0.5554                                                                                                 \n",
      "\n",
      "Epoch 9/20                                                                                                                                                                                \n",
      "765/765 - 37s - loss: 0.2874 - accuracy: 0.6045 - val_loss: 0.3332 - val_accuracy: 0.5574                                                                                                 \n",
      "\n",
      "Epoch 10/20                                                                                                                                                                               \n",
      "765/765 - 36s - loss: 0.2827 - accuracy: 0.6127 - val_loss: 0.3267 - val_accuracy: 0.5405                                                                                                 \n",
      "\n",
      "Epoch 11/20                                                                                                                                                                               \n",
      "765/765 - 36s - loss: 0.2803 - accuracy: 0.6128 - val_loss: 0.3266 - val_accuracy: 0.5422                                                                                                 \n",
      "\n",
      "Epoch 12/20                                                                                                                                                                               \n",
      "765/765 - 36s - loss: 0.2814 - accuracy: 0.6179 - val_loss: 0.3319 - val_accuracy: 0.5410                                                                                                 \n",
      "\n",
      "Epoch 13/20                                                                                                                                                                               \n",
      "765/765 - 37s - loss: 0.2814 - accuracy: 0.6165 - val_loss: 0.3281 - val_accuracy: 0.5461                                                                                                 \n",
      "\n",
      "Epoch 14/20                                                                                                                                                                               \n",
      "765/765 - 36s - loss: 0.2811 - accuracy: 0.6188 - val_loss: 0.3303 - val_accuracy: 0.5397                                                                                                 \n",
      "\n",
      "Epoch 15/20                                                                                                                                                                               \n",
      "765/765 - 36s - loss: 0.2808 - accuracy: 0.6181 - val_loss: 0.3288 - val_accuracy: 0.5482                                                                                                 \n",
      "\n",
      "Epoch 16/20                                                                                                                                                                               \n",
      "765/765 - 37s - loss: 0.2786 - accuracy: 0.6234 - val_loss: 0.3302 - val_accuracy: 0.5551                                                                                                 \n",
      "\n",
      "Epoch 17/20                                                                                                                                                                               \n",
      "765/765 - 36s - loss: 0.2769 - accuracy: 0.6247 - val_loss: 0.3309 - val_accuracy: 0.5464                                                                                                 \n",
      "\n",
      "Epoch 18/20                                                                                                                                                                               \n",
      "765/765 - 37s - loss: 0.2740 - accuracy: 0.6280 - val_loss: 0.3282 - val_accuracy: 0.5415                                                                                                 \n",
      "\n",
      "Epoch 19/20                                                                                                                                                                               \n",
      "765/765 - 37s - loss: 0.2729 - accuracy: 0.6313 - val_loss: 0.3304 - val_accuracy: 0.5536                                                                                                 \n",
      "\n",
      "Epoch 20/20                                                                                                                                                                               \n",
      "765/765 - 37s - loss: 0.2735 - accuracy: 0.6288 - val_loss: 0.3332 - val_accuracy: 0.5495                                                                                                 \n",
      "\n",
      "Epoch 1/10                                                                                                                                                                                \n",
      "383/383 - 117s - loss: 0.3482 - accuracy: 0.4991 - val_loss: 0.3520 - val_accuracy: 0.5106                                                                                                \n",
      "\n",
      "Epoch 2/10                                                                                                                                                                                \n",
      "383/383 - 127s - loss: 0.3529 - accuracy: 0.5043 - val_loss: 0.3579 - val_accuracy: 0.5028                                                                                                \n",
      "\n",
      "Epoch 3/10                                                                                                                                                                                \n",
      "383/383 - 128s - loss: 0.3579 - accuracy: 0.5060 - val_loss: 0.3581 - val_accuracy: 0.5016                                                                                                \n",
      "\n",
      "Epoch 4/10                                                                                                                                                                                \n",
      "383/383 - 129s - loss: 0.3588 - accuracy: 0.5018 - val_loss: 0.3638 - val_accuracy: 0.5163                                                                                                \n",
      "\n",
      "Epoch 5/10                                                                                                                                                                                \n",
      "383/383 - 129s - loss: 0.3595 - accuracy: 0.5021 - val_loss: 0.3547 - val_accuracy: 0.4993                                                                                                \n",
      "\n",
      "Epoch 6/10                                                                                                                                                                                \n",
      "383/383 - 128s - loss: 0.3582 - accuracy: 0.4988 - val_loss: 0.3627 - val_accuracy: 0.4977                                                                                                \n",
      "\n",
      "Epoch 7/10                                                                                                                                                                                \n",
      "383/383 - 128s - loss: 0.3610 - accuracy: 0.4985 - val_loss: 0.3767 - val_accuracy: 0.5088                                                                                                \n",
      "\n",
      "Epoch 8/10                                                                                                                                                                                \n",
      "383/383 - 129s - loss: 0.3641 - accuracy: 0.4987 - val_loss: 0.3614 - val_accuracy: 0.5057                                                                                                \n",
      "\n",
      "Epoch 9/10                                                                                                                                                                                \n",
      "383/383 - 129s - loss: 0.3640 - accuracy: 0.4996 - val_loss: 0.3664 - val_accuracy: 0.4993                                                                                                \n",
      "\n",
      "Epoch 10/10                                                                                                                                                                               \n",
      "383/383 - 129s - loss: 0.3601 - accuracy: 0.4951 - val_loss: 0.3688 - val_accuracy: 0.5013                                                                                                \n",
      "\n",
      "Epoch 1/10                                                                                                                                                                                \n",
      "192/192 - 65s - loss: 0.9884 - accuracy: 0.5075 - val_loss: 0.9877 - val_accuracy: 0.5003                                                                                                 \n",
      "\n",
      "Epoch 2/10                                                                                                                                                                                \n",
      "192/192 - 60s - loss: 0.9855 - accuracy: 0.4979 - val_loss: 0.9697 - val_accuracy: 0.5023                                                                                                 \n",
      "\n",
      "Epoch 3/10                                                                                                                                                                                \n",
      "192/192 - 61s - loss: 0.9841 - accuracy: 0.4969 - val_loss: 0.9753 - val_accuracy: 0.5036                                                                                                 \n",
      "\n",
      "Epoch 4/10                                                                                                                                                                                \n",
      "192/192 - 60s - loss: 0.9845 - accuracy: 0.4984 - val_loss: 0.9860 - val_accuracy: 0.4992                                                                                                 \n",
      "\n",
      "Epoch 5/10                                                                                                                                                                                \n",
      "192/192 - 61s - loss: 0.9850 - accuracy: 0.4976 - val_loss: 0.9860 - val_accuracy: 0.5021                                                                                                 \n",
      "\n",
      "Epoch 6/10                                                                                                                                                                                \n",
      "192/192 - 60s - loss: 0.9837 - accuracy: 0.4961 - val_loss: 0.9841 - val_accuracy: 0.4989                                                                                                 \n",
      "\n",
      "Epoch 7/10                                                                                                                                                                                \n",
      "192/192 - 61s - loss: 0.9839 - accuracy: 0.4989 - val_loss: 0.9781 - val_accuracy: 0.4995                                                                                                 \n",
      "\n",
      "Epoch 8/10                                                                                                                                                                                \n",
      "192/192 - 60s - loss: 0.9832 - accuracy: 0.4985 - val_loss: 0.9788 - val_accuracy: 0.5020                                                                                                 \n",
      "\n",
      "Epoch 9/10                                                                                                                                                                                \n",
      "192/192 - 60s - loss: 0.9838 - accuracy: 0.5023 - val_loss: 0.9622 - val_accuracy: 0.5013                                                                                                 \n",
      "\n",
      "Epoch 10/10                                                                                                                                                                               \n",
      "192/192 - 60s - loss: 0.9810 - accuracy: 0.4991 - val_loss: 0.9830 - val_accuracy: 0.4989                                                                                                 \n",
      "\n",
      "Epoch 1/20                                                                                                                                                                                \n",
      "96/96 - 77s - loss: 0.3750 - accuracy: 0.5062 - val_loss: 0.3576 - val_accuracy: 0.5018                                                                                                   \n",
      "\n",
      "Epoch 2/20                                                                                                                                                                                \n",
      "96/96 - 79s - loss: 0.3565 - accuracy: 0.4968 - val_loss: 0.3642 - val_accuracy: 0.5093                                                                                                   \n",
      "\n",
      "Epoch 3/20                                                                                                                                                                                \n",
      "96/96 - 82s - loss: 0.3600 - accuracy: 0.5012 - val_loss: 0.3641 - val_accuracy: 0.4964                                                                                                   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20                                                                                                                                                                                \n",
      "96/96 - 85s - loss: 0.3543 - accuracy: 0.4984 - val_loss: 0.3538 - val_accuracy: 0.5018                                                                                                   \n",
      "\n",
      "Epoch 5/20                                                                                                                                                                                \n",
      "96/96 - 85s - loss: 0.3584 - accuracy: 0.5045 - val_loss: 0.3610 - val_accuracy: 0.5034                                                                                                   \n",
      "\n",
      "Epoch 6/20                                                                                                                                                                                \n",
      "96/96 - 87s - loss: 0.3616 - accuracy: 0.5014 - val_loss: 0.3613 - val_accuracy: 0.4982                                                                                                   \n",
      "\n",
      "Epoch 7/20                                                                                                                                                                                \n",
      "96/96 - 87s - loss: 0.3582 - accuracy: 0.4934 - val_loss: 0.3605 - val_accuracy: 0.4956                                                                                                   \n",
      "\n",
      "Epoch 8/20                                                                                                                                                                                \n",
      "96/96 - 88s - loss: 0.3620 - accuracy: 0.4975 - val_loss: 0.3631 - val_accuracy: 0.4931                                                                                                   \n",
      "\n",
      "Epoch 9/20                                                                                                                                                                                \n",
      "96/96 - 87s - loss: 0.3594 - accuracy: 0.4958 - val_loss: 0.3691 - val_accuracy: 0.5078                                                                                                   \n",
      "\n",
      "Epoch 10/20                                                                                                                                                                               \n",
      "96/96 - 87s - loss: 0.3640 - accuracy: 0.5030 - val_loss: 0.3633 - val_accuracy: 0.4984                                                                                                   \n",
      "\n",
      "Epoch 11/20                                                                                                                                                                               \n",
      "96/96 - 87s - loss: 0.3613 - accuracy: 0.4955 - val_loss: 0.3630 - val_accuracy: 0.4936                                                                                                   \n",
      "\n",
      "Epoch 12/20                                                                                                                                                                               \n",
      "96/96 - 87s - loss: 0.3680 - accuracy: 0.5041 - val_loss: 0.3696 - val_accuracy: 0.4989                                                                                                   \n",
      "\n",
      "Epoch 13/20                                                                                                                                                                               \n",
      "96/96 - 89s - loss: 0.3642 - accuracy: 0.4961 - val_loss: 0.3713 - val_accuracy: 0.5077                                                                                                   \n",
      "\n",
      "Epoch 14/20                                                                                                                                                                               \n",
      "96/96 - 93s - loss: 0.3634 - accuracy: 0.4962 - val_loss: 0.3690 - val_accuracy: 0.5064                                                                                                   \n",
      "\n",
      "Epoch 15/20                                                                                                                                                                               \n",
      "96/96 - 88s - loss: 0.3644 - accuracy: 0.4950 - val_loss: 0.3691 - val_accuracy: 0.4926                                                                                                   \n",
      "\n",
      "Epoch 16/20                                                                                                                                                                               \n",
      "96/96 - 87s - loss: 0.3654 - accuracy: 0.4960 - val_loss: 0.3730 - val_accuracy: 0.5036                                                                                                   \n",
      "\n",
      "Epoch 17/20                                                                                                                                                                               \n",
      "96/96 - 87s - loss: 0.3685 - accuracy: 0.4997 - val_loss: 0.3737 - val_accuracy: 0.5105                                                                                                   \n",
      "\n",
      "Epoch 18/20                                                                                                                                                                               \n",
      "96/96 - 87s - loss: 0.3616 - accuracy: 0.4894 - val_loss: 0.3714 - val_accuracy: 0.5029                                                                                                   \n",
      "\n",
      "Epoch 19/20                                                                                                                                                                               \n",
      "96/96 - 87s - loss: 0.3665 - accuracy: 0.4981 - val_loss: 0.3680 - val_accuracy: 0.4989                                                                                                   \n",
      "\n",
      "Epoch 20/20                                                                                                                                                                               \n",
      "96/96 - 88s - loss: 0.3690 - accuracy: 0.4999 - val_loss: 0.3800 - val_accuracy: 0.5144                                                                                                   \n",
      "\n",
      "Epoch 1/10                                                                                                                                                                                \n",
      "192/192 - 108s - loss: 1.0867 - accuracy: 0.5031 - val_loss: 1.0563 - val_accuracy: 0.5116                                                                                                \n",
      "\n",
      "Epoch 2/10                                                                                                                                                                                \n",
      "192/192 - 102s - loss: 1.1285 - accuracy: 0.5033 - val_loss: 1.3203 - val_accuracy: 0.5025                                                                                                \n",
      "\n",
      "Epoch 3/10                                                                                                                                                                                \n",
      "192/192 - 104s - loss: 2.3009 - accuracy: 0.5139 - val_loss: 1.3744 - val_accuracy: 0.5178                                                                                                \n",
      "\n",
      "Epoch 4/10                                                                                                                                                                                \n",
      "192/192 - 122s - loss: 4.0852 - accuracy: 0.5057 - val_loss: 1.8164 - val_accuracy: 0.5059                                                                                                \n",
      "\n",
      "Epoch 5/10                                                                                                                                                                                \n",
      "192/192 - 122s - loss: 2.4344 - accuracy: 0.5003 - val_loss: 1.8900 - val_accuracy: 0.5219                                                                                                \n",
      "\n",
      "Epoch 6/10                                                                                                                                                                                \n",
      "192/192 - 122s - loss: 1.7778 - accuracy: 0.5020 - val_loss: 1.6272 - val_accuracy: 0.5170                                                                                                \n",
      "\n",
      "Epoch 7/10                                                                                                                                                                                \n",
      "192/192 - 124s - loss: 1.7716 - accuracy: 0.5079 - val_loss: 1.8595 - val_accuracy: 0.5224                                                                                                \n",
      "\n",
      "Epoch 8/10                                                                                                                                                                                \n",
      "192/192 - 123s - loss: 4.1968 - accuracy: 0.5211 - val_loss: 4.2592 - val_accuracy: 0.5358                                                                                                \n",
      "\n",
      "Epoch 9/10                                                                                                                                                                                \n",
      "192/192 - 122s - loss: 2.9429 - accuracy: 0.5180 - val_loss: 1.7616 - val_accuracy: 0.5237                                                                                                \n",
      "\n",
      "Epoch 10/10                                                                                                                                                                               \n",
      "192/192 - 123s - loss: 1.7445 - accuracy: 0.5041 - val_loss: 1.6831 - val_accuracy: 0.5082                                                                                                \n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [1:32:01<00:00, 1104.39s/trial, best loss: -0.5573529601097107]\n"
     ]
    }
   ],
   "source": [
    "def try_hyperas():\n",
    "    try:\n",
    "        best_run, best_model, space = optim.minimize(model=create_model,\n",
    "                                                     data=data,\n",
    "                                                     algo=tpe.suggest,\n",
    "                                                     max_evals=5,\n",
    "                                                     trials=Trials(),\n",
    "                                                     eval_space=True,\n",
    "                                                     notebook_name='hyperas_optimization',\n",
    "                                                     return_space=True)\n",
    "        \n",
    "        return best_run, best_model, space\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e.message)\n",
    "        print(e)\n",
    "        \n",
    "        return None, None, None \n",
    "        \n",
    "best_run, best_model, space = try_hyperas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65563786",
   "metadata": {},
   "source": [
    "### Melhores resultados\n",
    "Resultado dos melhores hiperparâmetros encontrados assim como o evaluate dos valores de loss e métricas do modelo treinado.\n",
    "\n",
    "Como é apresentado no [link](https://stackoverflow.com/questions/44843581/what-is-the-difference-between-model-fit-an-model-evaluate-in-keras) diferença entre:\n",
    "\n",
    "- *fit()*: is for training the model with the given inputs (and corresponding training labels);\n",
    "\n",
    "- *evaluate()*: is for evaluating the already trained model using the validation (or test) data and the corresponding labels. Returns the loss value and metrics values for the model;\n",
    "\n",
    "- *predict()*: is for the actual prediction. It generates output predictions for the input samples.\n",
    "\n",
    "Logo o hyperas realiza a avaliação do modelo treinado e não a predição de dados dado como entrada e com resultados de saída, o qual no caso do meu trabalho seriam valores entre 0 e 1, onde quanto mais próximo de 1 mais similar são duas instâncias de textos indicando que possivelmente são de mesmo autores. Caso contrário são assimilares e de autores distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "325f7365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutation of best performing model:\n",
      "107/107 [==============================] - 5s 43ms/step - loss: 0.3219 - accuracy: 0.5626\n",
      "[0.3219170868396759, 0.5626470446586609]\n",
      "Best performing model chosen hyperparameters:\n",
      "{'Activation': 'selu', 'Dropout': 0.11245974166556161, 'Dropout_1': 0.3207527760045966, 'LSTM': 128, 'batch_size': 32, 'epochs': 20, 'kernel_initializer': <tensorflow.python.keras.initializers.initializers_v2.VarianceScaling object at 0x0000024506657710>, 'loss': <tensorflow.python.keras.losses.MeanSquaredError object at 0x00000245065FAC88>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x00000244FE6C4F28>}\n"
     ]
    }
   ],
   "source": [
    "if best_run == None:\n",
    "    print(\"It was not possible to perform your best model hyperparameters!\")\n",
    "else:\n",
    "    x_train, y_train, x_test, y_test = data()\n",
    "    \n",
    "    print(\"Evalutation of best performing model:\")\n",
    "    print(best_model.evaluate([x_test['left'], x_test['right']], y_test))\n",
    "    \n",
    "    print(\"Best performing model chosen hyperparameters:\")\n",
    "    print(best_run)\n",
    "    \n",
    "    # realizar aqui o código que guarda em um arquivo .CSV os melhores hyperparâmetros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
