{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd35d3c9",
   "metadata": {},
   "source": [
    "### Instalação das dependências\n",
    "Como este notebook está dentro do projeto do TCC é necessário instalar somente o hyperas. Caso não tenha as outras dependências basta rodar o arquivo de **requirements.txt** presente na raiz da aplicação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd23f2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperas in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (0.4.1)\n",
      "Requirement already satisfied: nbconvert in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from hyperas) (6.0.7)\n",
      "Requirement already satisfied: jupyter in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from hyperas) (1.0.0)\n",
      "Requirement already satisfied: keras in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from hyperas) (2.4.3)\n",
      "Requirement already satisfied: entrypoints in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from hyperas) (0.3)\n",
      "Requirement already satisfied: nbformat in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from hyperas) (5.1.3)\n",
      "Requirement already satisfied: hyperopt in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from hyperas) (0.2.5)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbconvert->hyperas) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbconvert->hyperas) (1.4.3)\n",
      "Requirement already satisfied: defusedxml in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbconvert->hyperas) (0.7.1)\n",
      "Requirement already satisfied: bleach in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbconvert->hyperas) (3.3.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbconvert->hyperas) (4.3.3)\n",
      "Requirement already satisfied: jupyter-core in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbconvert->hyperas) (4.7.1)\n",
      "Requirement already satisfied: testpath in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbconvert->hyperas) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbconvert->hyperas) (0.1.2)\n",
      "Requirement already satisfied: jinja2>=2.4 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbconvert->hyperas) (2.11.2)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbconvert->hyperas) (2.9.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbconvert->hyperas) (0.5.3)\n",
      "Requirement already satisfied: ipykernel in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from jupyter->hyperas) (5.5.5)\n",
      "Requirement already satisfied: qtconsole in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from jupyter->hyperas) (5.1.1)\n",
      "Requirement already satisfied: notebook in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from jupyter->hyperas) (6.4.0)\n",
      "Requirement already satisfied: jupyter-console in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from jupyter->hyperas) (6.4.0)\n",
      "Requirement already satisfied: ipywidgets in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from jupyter->hyperas) (7.6.3)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from keras->hyperas) (1.4.1)\n",
      "Requirement already satisfied: h5py in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from keras->hyperas) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from keras->hyperas) (1.18.1)\n",
      "Requirement already satisfied: pyyaml in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from keras->hyperas) (5.3.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbformat->hyperas) (3.2.0)\n",
      "Requirement already satisfied: ipython-genutils in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbformat->hyperas) (0.2.0)\n",
      "Requirement already satisfied: cloudpickle in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from hyperopt->hyperas) (1.6.0)\n",
      "Requirement already satisfied: tqdm in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from hyperopt->hyperas) (4.48.0)\n",
      "Requirement already satisfied: six in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from hyperopt->hyperas) (1.14.0)\n",
      "Requirement already satisfied: future in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from hyperopt->hyperas) (0.18.2)\n",
      "Requirement already satisfied: networkx>=2.2 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from hyperopt->hyperas) (2.5.1)\n",
      "Requirement already satisfied: webencodings in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
      "Requirement already satisfied: packaging in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from bleach->nbconvert->hyperas) (21.0)\n",
      "Requirement already satisfied: decorator in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from traitlets>=4.2->nbconvert->hyperas) (4.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from jinja2>=2.4->nbconvert->hyperas) (1.1.1)\n",
      "Requirement already satisfied: nest-asyncio in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->hyperas) (1.5.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->hyperas) (6.1.12)\n",
      "Requirement already satisfied: async-generator in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->hyperas) (1.10)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from ipykernel->jupyter->hyperas) (7.16.1)\n",
      "Requirement already satisfied: tornado>=4.2 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from ipykernel->jupyter->hyperas) (6.1)\n",
      "Requirement already satisfied: pyzmq>=17.1 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from qtconsole->jupyter->hyperas) (22.1.0)\n",
      "Requirement already satisfied: qtpy in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from qtconsole->jupyter->hyperas) (1.9.0)\n",
      "Requirement already satisfied: argon2-cffi in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from notebook->jupyter->hyperas) (20.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from notebook->jupyter->hyperas) (1.7.1)\n",
      "Requirement already satisfied: prometheus-client in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from notebook->jupyter->hyperas) (0.11.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from notebook->jupyter->hyperas) (0.10.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from jupyter-console->jupyter->hyperas) (3.0.19)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from ipywidgets->jupyter->hyperas) (3.5.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from ipywidgets->jupyter->hyperas) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->hyperas) (4.6.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: attrs>=17.4.0 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->hyperas) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->hyperas) (0.18.0)\n",
      "Requirement already satisfied: setuptools in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->hyperas) (49.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from packaging->bleach->nbconvert->hyperas) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert->hyperas) (2.8.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperas) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperas) (0.7.5)\n",
      "Requirement already satisfied: backcall in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperas) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from ipython>=5.0.0->ipykernel->jupyter->hyperas) (0.18.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from argon2-cffi->notebook->jupyter->hyperas) (1.14.6)\n",
      "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from terminado>=0.8.3->notebook->jupyter->hyperas) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->hyperas) (0.2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat->hyperas) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat->hyperas) (3.10.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter->hyperas) (0.8.2)\n",
      "Requirement already satisfied: pycparser in /home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/lib/python3.6/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter->hyperas) (2.20)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/heik/PycharmProjects/TCC-Rede-Neural-Siamesa-LSTM/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2ca93",
   "metadata": {},
   "source": [
    "### Importações das dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679fd163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Input, Embedding, LSTM, Lambda, Conv1D, Dense, Dropout, Activation, Bidirectional\n",
    "from tensorflow.python.keras import backend as k\n",
    "from tensorflow.python.keras.layers import Lambda, Reshape, dot\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4bddc",
   "metadata": {},
   "source": [
    "### Definição dos paths padrões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dbf61d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_FILES_INDEX_VECTORS_PATH = os.path.join(os.path.dirname(os.path.abspath(\"\")), \"data\", \"processed/index_vectors\")\n",
    "DATA_FILES_EMBEDDING_MATRICES_PATH = os.path.join(os.path.dirname(os.path.abspath(\"\")), \"data\", \"processed/embedding_matrices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1d05f",
   "metadata": {},
   "source": [
    "### Carregando os vetores de índices\n",
    "São os vetores que são alimentados na entrada da rede neural siamesa, onde cada valor representa **o índice correspondente na matriz de incoporação**. Ambos (matriz de incoporação e vetor de índice) são armazenados na aplicação após ser aplicado o processo de estruturação de dados. Para cada word embedding utilizado foi criado um arquivo que armazena os vetores de índices para os datasets: cru (raw), sem stopwords (sw) e sem stopwords e com lematização (sw_lemmatization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31042930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    ### FUNÇÕES INTERNAS ###\n",
    "    \n",
    "    def load_index_vector_dataframe(filename):\n",
    "        dataframe = pd.read_csv(filename)\n",
    "\n",
    "        for q in ['phrase1', 'phrase2']:\n",
    "            dataframe[q + '_n'] = dataframe[q].apply(lambda x: [int(i) for i in x.replace('[', '').replace(']', '').split(',')])\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    def split_data_train(train_dataframe):\n",
    "        x_phrases = train_dataframe[['phrase1_n', 'phrase2_n']]\n",
    "\n",
    "        train_dataframe.label = pd.Categorical(train_dataframe.label)\n",
    "        train_dataframe['label'] = train_dataframe.label.cat.codes\n",
    "        y_labels = train_dataframe['label']\n",
    "\n",
    "        return x_phrases, y_labels\n",
    "    \n",
    "    def split_and_zero_padding(dataframe, max_seq_length):\n",
    "        # Split to dicts\n",
    "        side_phrases = {'left': dataframe['phrase1_n'], 'right': dataframe['phrase2_n']}\n",
    "        dataset = None\n",
    "\n",
    "        # Zero padding\n",
    "        for dataset, side in itertools.product([side_phrases], ['left', 'right']):\n",
    "            dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    ### FUNÇÕES INTERNAS ###\n",
    "    \n",
    "    \n",
    "    ### ARQUIVOS VETOR INDICES ###\n",
    "\n",
    "    DATA_FILES_INDEX_VECTORS_PATH = os.path.join(os.path.dirname(os.path.abspath(\"\")), \"data\", \"processed/index_vectors\")\n",
    "    \n",
    "    # Word2vec Google News\n",
    "    raw_w2v_GN = os.path.join(DATA_FILES_INDEX_VECTORS_PATH, \"training-raw-w2v_GN.csv\")\n",
    "    sw_w2v_GN = os.path.join(DATA_FILES_INDEX_VECTORS_PATH, \"training-sw-w2v_GN.csv\")\n",
    "    sw_lemma_w2v_GN = os.path.join(DATA_FILES_INDEX_VECTORS_PATH, \"training-sw-lemmatization-w2v_GN.csv\")\n",
    "    \n",
    "    # Word2vec Wikipedia\n",
    "    \n",
    "    # Glove Wikipedia + Gigaword\n",
    "    \n",
    "    # Glove Common Crawl\n",
    "    \n",
    "    ### ARQUIVOS WORD EMBEDDING ###\n",
    "    \n",
    "    \n",
    "    ### TIPOS DE MAX_SEQ_LENGTH ###\n",
    "\n",
    "    MAX_SEQ_LENGTH_RAW = 17\n",
    "    MAX_SEQ_LENGTH_SW = 9\n",
    "    MAX_SEQ_LENGTH_SW_LEMMA = 9\n",
    "\n",
    "    max_seq_length = MAX_SEQ_LENGTH_RAW\n",
    "\n",
    "    ### TIPOS DE MAX_SEQ_LENGTH ###\n",
    "    \n",
    "    \n",
    "    # Carregamento do vetor de índices através do pandas    \n",
    "    train_dataframe = load_index_vector_dataframe(raw_w2v_GN)\n",
    "    \n",
    "    # Divisão do dataset de entrada da rede neural em TREINAMENTO e TESTE (PREDIÇÃO)\n",
    "    x_phrases, y_labels = split_data_train(train_dataframe)\n",
    "    \n",
    "    # Divisão 90/10 de treinamento e teste\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_phrases, y_labels, test_size=0.1, random_state=0, stratify=train_dataframe['label'])\n",
    "    \n",
    "    # Divisão dos dados de TREINAMENTO de entrada entre a parte esquerda e direita das subredes e zero padding à esquerda dos dados de TREINAMENTO (retorno em ndarray)\n",
    "    x_train = split_and_zero_padding(x_train, max_seq_length)\n",
    "\n",
    "    # Divisão dos dados de TESTE (predição) de entrada entre a parte esquerda e direita das subredes e zero padding à esquerda dos dados de TESTE (predição) (retorno em ndarray)\n",
    "    x_test = split_and_zero_padding(x_test, max_seq_length)\n",
    "\n",
    "    # Conversão para numpy das labels\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5804b1",
   "metadata": {},
   "source": [
    "### Criação do modelo da rede siamesa\n",
    "Criação do modelo da rede siamesa utilizando como subredes internas a arquitetura *LSTM (Long Short Term Memory)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd89b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    ### FUNÇÕES INTERNAS ###\n",
    "\n",
    "    # Distância de Manhattan\n",
    "    def define_manhattan_model(shared_model, max_seq_length):\n",
    "        def calculate_manhattan_distance(left_output, right_output):          \n",
    "            def __exponent_neg_manhattan_distance(left, right):\n",
    "                return k.exp(-k.sum(k.abs(left - right), axis=1, keepdims=True))\n",
    "\n",
    "            manhattan_distance = Lambda(\n",
    "                function=lambda x: __exponent_neg_manhattan_distance(x[0], x[1]),\n",
    "                output_shape=lambda x: (x[0][0], 1)\n",
    "            )([left_output, right_output])\n",
    "\n",
    "            return manhattan_distance\n",
    "\n",
    "        # The visible layer\n",
    "        left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "        right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "        # Pack it all up into a Manhattan Distance model\n",
    "        malstm_distance = calculate_manhattan_distance(shared_model(left_input), shared_model(right_input))\n",
    "        manhattan_model = Model(inputs=[left_input, right_input], outputs=[malstm_distance])\n",
    "\n",
    "        return manhattan_model\n",
    "\n",
    "    # Similaridade de Cossenos\n",
    "    def define_cosine_model(shared_model, max_seq_length):\n",
    "        def calculate_cosine_distance(left_output, right_output):\n",
    "            cos_distance = dot([left_output, right_output], axes=1, normalize=True)\n",
    "            cos_distance = Reshape((1,))(cos_distance)\n",
    "            cos_similarity = Lambda(lambda x: 1 - x)(cos_distance)\n",
    "\n",
    "            return cos_similarity\n",
    "\n",
    "        left_input = Input(shape=(max_seq_length,))\n",
    "        right_input = Input(shape=(max_seq_length,))\n",
    "\n",
    "        cosine_distance = calculate_cosine_distance(shared_model(left_input), shared_model(right_input))\n",
    "        cosine_model = Model(inputs=[left_input, right_input], outputs=[cosine_distance])\n",
    "\n",
    "        return cosine_model\n",
    "\n",
    "    # Distância Euclidiana\n",
    "    def define_euclidean_model(shared_model, max_seq_length):\n",
    "        def calculate_euclidean_distance(vects):\n",
    "            x, y = vects\n",
    "            sum_square = k.sum(k.square(x - y), axis=1, keepdims=True)\n",
    "\n",
    "            return k.sqrt(k.maximum(sum_square, k.epsilon()))\n",
    "\n",
    "        def dist_output_shape(shapes):\n",
    "            shape1, shape2 = shapes\n",
    "\n",
    "            return (shape1[0], 1)\n",
    "\n",
    "        left_input = Input(shape=(max_seq_length,))\n",
    "        right_input = Input(shape=(max_seq_length,))\n",
    "\n",
    "        euclidean_distance = Lambda(\n",
    "            calculate_euclidean_distance,\n",
    "            output_shape=dist_output_shape\n",
    "        )([shared_model(left_input), shared_model(right_input)])\n",
    "        euclidean_model = Model(inputs=[left_input, right_input], outputs=[euclidean_distance])\n",
    "\n",
    "        return euclidean_model\n",
    "\n",
    "    ### FUNÇÕES INTERNAS ###\n",
    "\n",
    "\n",
    "    ### TIPOS DE MAX_SEQ_LENGTH ###\n",
    "\n",
    "    MAX_SEQ_LENGTH_RAW = 17\n",
    "    MAX_SEQ_LENGTH_SW = 9\n",
    "    MAX_SEQ_LENGTH_SW_LEMMA = 9\n",
    "\n",
    "    max_seq_length = MAX_SEQ_LENGTH_RAW\n",
    "\n",
    "    ### TIPOS DE MAX_SEQ_LENGTH ###\n",
    "\n",
    "\n",
    "    ### ARQUIVOS EMBEDDINGS ###\n",
    "    \n",
    "    DATA_FILES_EMBEDDING_MATRICES_PATH = os.path.join(os.path.dirname(os.path.abspath(\"\")), \"data\", \"processed/embedding_matrices\")\n",
    "    \n",
    "    # Word2vec Google News\n",
    "    raw_w2v_GN = os.path.join(DATA_FILES_EMBEDDING_MATRICES_PATH, \"raw-w2v_GN.npy\")\n",
    "    sw_w2v_GN = os.path.join(DATA_FILES_EMBEDDING_MATRICES_PATH, \"sw-w2v_GN.npy\")\n",
    "    sw_lemma_w2v_GN = os.path.join(DATA_FILES_EMBEDDING_MATRICES_PATH, \"sw-lemmatization-w2v_GN.npy\")\n",
    "\n",
    "    # Word2vec Wikipedia\n",
    "\n",
    "    # Glove Wikipedia + Gigaword\n",
    "\n",
    "    # Glove Common Crawl\n",
    "\n",
    "    ### ARQUIVOS EMBEDDINGS ###\n",
    "\n",
    "\n",
    "    # Definição do modelo compartilhado (shared model) entre as subredes, pois são idênticas\n",
    "    embedding_dim = 300\n",
    "    embedding_matrix = np.load(raw_w2v_GN)\n",
    "\n",
    "    shared_model = Sequential()\n",
    "    shared_model.add(\n",
    "        Embedding(\n",
    "            len(embedding_matrix),\n",
    "            embedding_dim,\n",
    "            weights=[embedding_matrix],\n",
    "            input_shape=(max_seq_length,),\n",
    "            trainable=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    shared_model.add(Dropout({{uniform(0, 1)}}))\n",
    "    shared_model.add(Bidirectional(LSTM(\n",
    "        {{choice([64, 128, 256, 512])}},\n",
    "        kernel_initializer={{choice([tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='truncated_normal',seed=1), tf.keras.initializers.glorot_normal(seed=1)])}},\n",
    "        activation='softsign',\n",
    "        recurrent_activation='sigmoid',\n",
    "        dropout=0.0,\n",
    "        recurrent_dropout={{uniform(0, 1)}},\n",
    "        implementation=1\n",
    "    )))\n",
    "    shared_model.add(Activation({{choice(['relu', 'selu', 'elu'])}}))\n",
    "    shared_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Define o lado esquerdo e direito das subredes a partir da shared_model. Também define a medida/função de similaridade usada na camada de merge na saída da rede\n",
    "    model = define_manhattan_model(shared_model, max_seq_length)\n",
    "\n",
    "    # Compilação do modelo da rede siamesa\n",
    "    model.compile(\n",
    "        loss={{choice([tf.keras.losses.MeanSquaredError(), tfa.losses.ContrastiveLoss(), tfa.losses.TripletSemiHardLoss(), tfa.losses.TripletHardLoss()])}},\n",
    "        optimizer={{choice([tf.keras.optimizers.Adadelta(learning_rate=0.1, rho=0.95, epsilon=1e-07, name='Adadelta', clipnorm=1.5),\n",
    "                            tf.keras.optimizers.Adamax(learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"),\n",
    "                            tf.keras.optimizers.Adagrad(learning_rate=0.1,  initial_accumulator_value=0.1,epsilon=1e-07, name='Adagrad', clipnorm=1.5),\n",
    "                            tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.0, nesterov=False, name='SGD'),\n",
    "                            tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "                            tf.keras.optimizers.RMSprop(learning_rate=0.1)])}},\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Execução do treinamento da rede\n",
    "    training_history = model.fit(\n",
    "        [x_train['left'], x_train['right']],\n",
    "        y_train,\n",
    "        batch_size={{choice([16, 32, 64, 128, 256])}},\n",
    "        epochs={{choice([10, 20])}},\n",
    "        verbose=2,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    return {'loss': -(np.amax(training_history.history['val_accuracy'])), 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c52199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partes substituidas para teste\n",
    "# {{choice([50, 100, 150, 200])}}\n",
    "# optimizer= tf.keras.optimizers.Adam(learning_rate={{choice([0.001, 0.01, 0.1])}}),\n",
    "# batch_size={{choice([8, 16, 32, 64, 128, 256])}},\n",
    "#epochs=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413f547",
   "metadata": {},
   "source": [
    "### Execução do Hyperas\n",
    "Execução das funções com os códigos básicos do Hyperas retornando o melhor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff98a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import itertools\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow_addons as tfa\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras.models import Model, Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras.layers import Input, Embedding, LSTM, Lambda, Conv1D, Dense, Dropout, Activation, Bidirectional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras import backend as k\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras.layers import Lambda, Reshape, dot\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'LSTM': hp.choice('LSTM', [64, 128, 256, 512]),\n",
      "        'kernel_initializer': hp.choice('kernel_initializer', [tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='truncated_normal',seed=1), tf.keras.initializers.glorot_normal(seed=1)]),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'Activation': hp.choice('Activation', ['relu', 'selu', 'elu']),\n",
      "        'loss': hp.choice('loss', [tf.keras.losses.MeanSquaredError(), tfa.losses.ContrastiveLoss(), tfa.losses.TripletSemiHardLoss(), tfa.losses.TripletHardLoss()]),\n",
      "        'optimizer': hp.choice('optimizer', [tf.keras.optimizers.Adadelta(learning_rate=0.1, rho=0.95, epsilon=1e-07, name='Adadelta', clipnorm=1.5),\n",
      "                            tf.keras.optimizers.Adamax(learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"),\n",
      "                            tf.keras.optimizers.Adagrad(learning_rate=0.1,  initial_accumulator_value=0.1,epsilon=1e-07, name='Adagrad', clipnorm=1.5),\n",
      "                            tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.0, nesterov=False, name='SGD'),\n",
      "                            tf.keras.optimizers.Adam(learning_rate=0.1),\n",
      "                            tf.keras.optimizers.RMSprop(learning_rate=0.1)]),\n",
      "        'batch_size': hp.choice('batch_size', [16, 32, 64, 128, 256]),\n",
      "        'epochs': hp.choice('epochs', [10, 20]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: ### FUNÇÕES INTERNAS ###\n",
      "   3: \n",
      "   4: def load_index_vector_dataframe(filename):\n",
      "   5:     dataframe = pd.read_csv(filename)\n",
      "   6: \n",
      "   7:     for q in ['phrase1', 'phrase2']:\n",
      "   8:         dataframe[q + '_n'] = dataframe[q].apply(lambda x: [int(i) for i in x.replace('[', '').replace(']', '').split(',')])\n",
      "   9: \n",
      "  10:     return dataframe\n",
      "  11: \n",
      "  12: def split_data_train(train_dataframe):\n",
      "  13:     x_phrases = train_dataframe[['phrase1_n', 'phrase2_n']]\n",
      "  14: \n",
      "  15:     train_dataframe.label = pd.Categorical(train_dataframe.label)\n",
      "  16:     train_dataframe['label'] = train_dataframe.label.cat.codes\n",
      "  17:     y_labels = train_dataframe['label']\n",
      "  18: \n",
      "  19:     return x_phrases, y_labels\n",
      "  20: \n",
      "  21: def split_and_zero_padding(dataframe, max_seq_length):\n",
      "  22:     # Split to dicts\n",
      "  23:     side_phrases = {'left': dataframe['phrase1_n'], 'right': dataframe['phrase2_n']}\n",
      "  24:     dataset = None\n",
      "  25: \n",
      "  26:     # Zero padding\n",
      "  27:     for dataset, side in itertools.product([side_phrases], ['left', 'right']):\n",
      "  28:         dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n",
      "  29: \n",
      "  30:     return dataset\n",
      "  31: \n",
      "  32: ### FUNÇÕES INTERNAS ###\n",
      "  33: \n",
      "  34: \n",
      "  35: ### ARQUIVOS VETOR INDICES ###\n",
      "  36: \n",
      "  37: DATA_FILES_INDEX_VECTORS_PATH = os.path.join(os.path.dirname(os.path.abspath(\"\")), \"data\", \"processed/index_vectors\")\n",
      "  38: \n",
      "  39: # Word2vec Google News\n",
      "  40: raw_w2v_GN = os.path.join(DATA_FILES_INDEX_VECTORS_PATH, \"training-raw-w2v_GN.csv\")\n",
      "  41: sw_w2v_GN = os.path.join(DATA_FILES_INDEX_VECTORS_PATH, \"training-sw-w2v_GN.csv\")\n",
      "  42: sw_lemma_w2v_GN = os.path.join(DATA_FILES_INDEX_VECTORS_PATH, \"training-sw-lemmatization-w2v_GN.csv\")\n",
      "  43: \n",
      "  44: # Word2vec Wikipedia\n",
      "  45: \n",
      "  46: # Glove Wikipedia + Gigaword\n",
      "  47: \n",
      "  48: # Glove Common Crawl\n",
      "  49: \n",
      "  50: ### ARQUIVOS WORD EMBEDDING ###\n",
      "  51: \n",
      "  52: \n",
      "  53: ### TIPOS DE MAX_SEQ_LENGTH ###\n",
      "  54: \n",
      "  55: MAX_SEQ_LENGTH_RAW = 17\n",
      "  56: MAX_SEQ_LENGTH_SW = 9\n",
      "  57: MAX_SEQ_LENGTH_SW_LEMMA = 9\n",
      "  58: \n",
      "  59: max_seq_length = MAX_SEQ_LENGTH_RAW\n",
      "  60: \n",
      "  61: ### TIPOS DE MAX_SEQ_LENGTH ###\n",
      "  62: \n",
      "  63: \n",
      "  64: # Carregamento do vetor de índices através do pandas    \n",
      "  65: train_dataframe = load_index_vector_dataframe(raw_w2v_GN)\n",
      "  66: \n",
      "  67: # Divisão do dataset de entrada da rede neural em TREINAMENTO e TESTE (PREDIÇÃO)\n",
      "  68: x_phrases, y_labels = split_data_train(train_dataframe)\n",
      "  69: \n",
      "  70: # Divisão 90/10 de treinamento e teste\n",
      "  71: x_train, x_test, y_train, y_test = train_test_split(x_phrases, y_labels, test_size=0.1, random_state=0, stratify=train_dataframe['label'])\n",
      "  72: \n",
      "  73: # Divisão dos dados de TREINAMENTO de entrada entre a parte esquerda e direita das subredes e zero padding à esquerda dos dados de TREINAMENTO (retorno em ndarray)\n",
      "  74: x_train = split_and_zero_padding(x_train, max_seq_length)\n",
      "  75: \n",
      "  76: # Divisão dos dados de TESTE (predição) de entrada entre a parte esquerda e direita das subredes e zero padding à esquerda dos dados de TESTE (predição) (retorno em ndarray)\n",
      "  77: x_test = split_and_zero_padding(x_test, max_seq_length)\n",
      "  78: \n",
      "  79: # Conversão para numpy das labels\n",
      "  80: y_train = y_train.values\n",
      "  81: y_test = y_test.values\n",
      "  82: \n",
      "  83: \n",
      "  84: \n",
      "  85: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \n",
      "   4:     ### FUNÇÕES INTERNAS ###\n",
      "   5: \n",
      "   6:     # Distância de Manhattan\n",
      "   7:     def define_manhattan_model(shared_model, max_seq_length):\n",
      "   8:         def calculate_manhattan_distance(left_output, right_output):          \n",
      "   9:             def __exponent_neg_manhattan_distance(left, right):\n",
      "  10:                 return k.exp(-k.sum(k.abs(left - right), axis=1, keepdims=True))\n",
      "  11: \n",
      "  12:             manhattan_distance = Lambda(\n",
      "  13:                 function=lambda x: __exponent_neg_manhattan_distance(x[0], x[1]),\n",
      "  14:                 output_shape=lambda x: (x[0][0], 1)\n",
      "  15:             )([left_output, right_output])\n",
      "  16: \n",
      "  17:             return manhattan_distance\n",
      "  18: \n",
      "  19:         # The visible layer\n",
      "  20:         left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
      "  21:         right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
      "  22: \n",
      "  23:         # Pack it all up into a Manhattan Distance model\n",
      "  24:         malstm_distance = calculate_manhattan_distance(shared_model(left_input), shared_model(right_input))\n",
      "  25:         manhattan_model = Model(inputs=[left_input, right_input], outputs=[malstm_distance])\n",
      "  26: \n",
      "  27:         return manhattan_model\n",
      "  28: \n",
      "  29:     # Similaridade de Cossenos\n",
      "  30:     def define_cosine_model(shared_model, max_seq_length):\n",
      "  31:         def calculate_cosine_distance(left_output, right_output):\n",
      "  32:             cos_distance = dot([left_output, right_output], axes=1, normalize=True)\n",
      "  33:             cos_distance = Reshape((1,))(cos_distance)\n",
      "  34:             cos_similarity = Lambda(lambda x: 1 - x)(cos_distance)\n",
      "  35: \n",
      "  36:             return cos_similarity\n",
      "  37: \n",
      "  38:         left_input = Input(shape=(max_seq_length,))\n",
      "  39:         right_input = Input(shape=(max_seq_length,))\n",
      "  40: \n",
      "  41:         cosine_distance = calculate_cosine_distance(shared_model(left_input), shared_model(right_input))\n",
      "  42:         cosine_model = Model(inputs=[left_input, right_input], outputs=[cosine_distance])\n",
      "  43: \n",
      "  44:         return cosine_model\n",
      "  45: \n",
      "  46:     # Distância Euclidiana\n",
      "  47:     def define_euclidean_model(shared_model, max_seq_length):\n",
      "  48:         def calculate_euclidean_distance(vects):\n",
      "  49:             x, y = vects\n",
      "  50:             sum_square = k.sum(k.square(x - y), axis=1, keepdims=True)\n",
      "  51: \n",
      "  52:             return k.sqrt(k.maximum(sum_square, k.epsilon()))\n",
      "  53: \n",
      "  54:         def dist_output_shape(shapes):\n",
      "  55:             shape1, shape2 = shapes\n",
      "  56: \n",
      "  57:             return (shape1[0], 1)\n",
      "  58: \n",
      "  59:         left_input = Input(shape=(max_seq_length,))\n",
      "  60:         right_input = Input(shape=(max_seq_length,))\n",
      "  61: \n",
      "  62:         euclidean_distance = Lambda(\n",
      "  63:             calculate_euclidean_distance,\n",
      "  64:             output_shape=dist_output_shape\n",
      "  65:         )([shared_model(left_input), shared_model(right_input)])\n",
      "  66:         euclidean_model = Model(inputs=[left_input, right_input], outputs=[euclidean_distance])\n",
      "  67: \n",
      "  68:         return euclidean_model\n",
      "  69: \n",
      "  70:     ### FUNÇÕES INTERNAS ###\n",
      "  71: \n",
      "  72: \n",
      "  73:     ### TIPOS DE MAX_SEQ_LENGTH ###\n",
      "  74: \n",
      "  75:     MAX_SEQ_LENGTH_RAW = 17\n",
      "  76:     MAX_SEQ_LENGTH_SW = 9\n",
      "  77:     MAX_SEQ_LENGTH_SW_LEMMA = 9\n",
      "  78: \n",
      "  79:     max_seq_length = MAX_SEQ_LENGTH_RAW\n",
      "  80: \n",
      "  81:     ### TIPOS DE MAX_SEQ_LENGTH ###\n",
      "  82: \n",
      "  83: \n",
      "  84:     ### ARQUIVOS EMBEDDINGS ###\n",
      "  85:     \n",
      "  86:     DATA_FILES_EMBEDDING_MATRICES_PATH = os.path.join(os.path.dirname(os.path.abspath(\"\")), \"data\", \"processed/embedding_matrices\")\n",
      "  87:     \n",
      "  88:     # Word2vec Google News\n",
      "  89:     raw_w2v_GN = os.path.join(DATA_FILES_EMBEDDING_MATRICES_PATH, \"raw-w2v_GN.npy\")\n",
      "  90:     sw_w2v_GN = os.path.join(DATA_FILES_EMBEDDING_MATRICES_PATH, \"sw-w2v_GN.npy\")\n",
      "  91:     sw_lemma_w2v_GN = os.path.join(DATA_FILES_EMBEDDING_MATRICES_PATH, \"sw-lemmatization-w2v_GN.npy\")\n",
      "  92: \n",
      "  93:     # Word2vec Wikipedia\n",
      "  94: \n",
      "  95:     # Glove Wikipedia + Gigaword\n",
      "  96: \n",
      "  97:     # Glove Common Crawl\n",
      "  98: \n",
      "  99:     ### ARQUIVOS EMBEDDINGS ###\n",
      " 100: \n",
      " 101: \n",
      " 102:     # Definição do modelo compartilhado (shared model) entre as subredes, pois são idênticas\n",
      " 103:     embedding_dim = 300\n",
      " 104:     embedding_matrix = np.load(raw_w2v_GN)\n",
      " 105: \n",
      " 106:     shared_model = Sequential()\n",
      " 107:     shared_model.add(\n",
      " 108:         Embedding(\n",
      " 109:             len(embedding_matrix),\n",
      " 110:             embedding_dim,\n",
      " 111:             weights=[embedding_matrix],\n",
      " 112:             input_shape=(max_seq_length,),\n",
      " 113:             trainable=False\n",
      " 114:         )\n",
      " 115:     )\n",
      " 116:     \n",
      " 117:     shared_model.add(Dropout(space['Dropout']))\n",
      " 118:     shared_model.add(Bidirectional(LSTM(\n",
      " 119:         space['LSTM'],\n",
      " 120:         kernel_initializer=space['kernel_initializer'],\n",
      " 121:         activation='softsign',\n",
      " 122:         recurrent_activation='sigmoid',\n",
      " 123:         dropout=0.0,\n",
      " 124:         recurrent_dropout=space['Dropout_1'],\n",
      " 125:         implementation=1\n",
      " 126:     )))\n",
      " 127:     shared_model.add(Activation(space['Activation']))\n",
      " 128:     shared_model.add(Dense(1, activation='sigmoid'))\n",
      " 129: \n",
      " 130:     # Define o lado esquerdo e direito das subredes a partir da shared_model. Também define a medida/função de similaridade usada na camada de merge na saída da rede\n",
      " 131:     model = define_manhattan_model(shared_model, max_seq_length)\n",
      " 132: \n",
      " 133:     # Compilação do modelo da rede siamesa\n",
      " 134:     model.compile(\n",
      " 135:         loss=space['loss'],\n",
      " 136:         optimizer=space['optimizer'],\n",
      " 137:         metrics=['accuracy']\n",
      " 138:     )\n",
      " 139: \n",
      " 140:     # Execução do treinamento da rede\n",
      " 141:     training_history = model.fit(\n",
      " 142:         [x_train['left'], x_train['right']],\n",
      " 143:         y_train,\n",
      " 144:         batch_size=space['batch_size'],\n",
      " 145:         epochs=space['epochs'],\n",
      " 146:         verbose=2,\n",
      " 147:         validation_split=0.2\n",
      " 148:     )\n",
      " 149:     \n",
      " 150:     return {'loss': -(np.amax(training_history.history['val_accuracy'])), 'status': STATUS_OK, 'model': model}\n",
      " 151: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20                                           \n",
      "765/765 - 133s - loss: 0.3228 - accuracy: 0.5350 - val_loss: 0.3165 - val_accuracy: 0.5495\n",
      "\n",
      "Epoch 2/20                                           \n",
      "765/765 - 126s - loss: 0.3063 - accuracy: 0.5632 - val_loss: 0.3294 - val_accuracy: 0.5208\n",
      "\n",
      "Epoch 3/20                                           \n",
      "765/765 - 125s - loss: 0.3006 - accuracy: 0.5738 - val_loss: 0.3183 - val_accuracy: 0.5462\n",
      "\n",
      "Epoch 4/20                                           \n",
      "765/765 - 127s - loss: 0.2933 - accuracy: 0.5880 - val_loss: 0.3216 - val_accuracy: 0.5412\n",
      "\n",
      "Epoch 5/20                                           \n",
      "765/765 - 126s - loss: 0.2900 - accuracy: 0.5933 - val_loss: 0.3162 - val_accuracy: 0.5596\n",
      "\n",
      "Epoch 6/20                                           \n",
      "765/765 - 126s - loss: 0.2898 - accuracy: 0.5929 - val_loss: 0.3212 - val_accuracy: 0.5515\n",
      "\n",
      "Epoch 7/20                                           \n",
      "765/765 - 126s - loss: 0.2828 - accuracy: 0.6062 - val_loss: 0.3254 - val_accuracy: 0.5502\n",
      "\n",
      "Epoch 8/20                                           \n",
      "765/765 - 126s - loss: 0.2827 - accuracy: 0.6078 - val_loss: 0.3242 - val_accuracy: 0.5528\n",
      "\n",
      "Epoch 9/20                                           \n",
      "765/765 - 126s - loss: 0.2864 - accuracy: 0.6010 - val_loss: 0.3211 - val_accuracy: 0.5474\n",
      "\n",
      "Epoch 10/20                                          \n",
      "765/765 - 126s - loss: 0.2824 - accuracy: 0.6058 - val_loss: 0.3202 - val_accuracy: 0.5462\n",
      "\n",
      "Epoch 11/20                                          \n",
      "765/765 - 126s - loss: 0.2804 - accuracy: 0.6106 - val_loss: 0.3228 - val_accuracy: 0.5554\n",
      "\n",
      "Epoch 12/20                                          \n",
      "765/765 - 124s - loss: 0.2809 - accuracy: 0.6138 - val_loss: 0.3223 - val_accuracy: 0.5490\n",
      "\n",
      "Epoch 13/20                                          \n",
      "765/765 - 125s - loss: 0.2796 - accuracy: 0.6133 - val_loss: 0.3325 - val_accuracy: 0.5485\n",
      "\n",
      "Epoch 14/20                                          \n",
      "765/765 - 126s - loss: 0.2761 - accuracy: 0.6168 - val_loss: 0.3277 - val_accuracy: 0.5449\n",
      "\n",
      "Epoch 15/20                                          \n",
      "765/765 - 126s - loss: 0.2765 - accuracy: 0.6182 - val_loss: 0.3248 - val_accuracy: 0.5454\n",
      "\n",
      "Epoch 16/20                                          \n",
      "765/765 - 126s - loss: 0.2736 - accuracy: 0.6208 - val_loss: 0.3235 - val_accuracy: 0.5495\n",
      "\n",
      "Epoch 17/20                                          \n",
      "765/765 - 126s - loss: 0.2700 - accuracy: 0.6298 - val_loss: 0.3199 - val_accuracy: 0.5490\n",
      "\n",
      "Epoch 18/20                                          \n",
      "765/765 - 130s - loss: 0.2740 - accuracy: 0.6232 - val_loss: 0.3285 - val_accuracy: 0.5538\n",
      "\n",
      "Epoch 19/20                                          \n",
      "765/765 - 124s - loss: 0.2714 - accuracy: 0.6274 - val_loss: 0.3220 - val_accuracy: 0.5511\n",
      "\n",
      "Epoch 20/20                                          \n",
      "765/765 - 126s - loss: 0.2671 - accuracy: 0.6334 - val_loss: 0.3256 - val_accuracy: 0.5608\n",
      "\n",
      "Epoch 1/10                                                                          \n",
      "383/383 - 106s - loss: 0.9829 - accuracy: 0.5009 - val_loss: 0.9817 - val_accuracy: 0.4892\n",
      "\n",
      "Epoch 2/10                                                                          \n",
      "383/383 - 105s - loss: 0.9816 - accuracy: 0.5019 - val_loss: 0.9787 - val_accuracy: 0.4979\n",
      "\n",
      "Epoch 3/10                                                                          \n",
      "383/383 - 106s - loss: 0.9808 - accuracy: 0.4931 - val_loss: 0.9783 - val_accuracy: 0.5186\n",
      "\n",
      "Epoch 4/10                                                                          \n",
      "383/383 - 105s - loss: 0.9773 - accuracy: 0.4982 - val_loss: 0.9745 - val_accuracy: 0.5020\n",
      "\n",
      "Epoch 5/10                                                                          \n",
      "383/383 - 104s - loss: 0.9752 - accuracy: 0.4999 - val_loss: 0.9736 - val_accuracy: 0.5046\n",
      "\n",
      "Epoch 6/10                                                                          \n",
      "383/383 - 105s - loss: 0.9754 - accuracy: 0.4989 - val_loss: 0.9752 - val_accuracy: 0.5093\n",
      "\n",
      "Epoch 7/10                                                                          \n",
      "383/383 - 105s - loss: 0.9768 - accuracy: 0.5004 - val_loss: 0.9772 - val_accuracy: 0.5018\n",
      "\n",
      "Epoch 8/10                                                                          \n",
      "383/383 - 105s - loss: 0.9763 - accuracy: 0.5011 - val_loss: 0.9775 - val_accuracy: 0.5082\n",
      "\n",
      "Epoch 9/10                                                                          \n",
      "383/383 - 105s - loss: 0.9761 - accuracy: 0.4978 - val_loss: 0.9758 - val_accuracy: 0.5212\n",
      "\n",
      "Epoch 10/10                                                                         \n",
      "383/383 - 105s - loss: 0.9746 - accuracy: 0.5067 - val_loss: 0.9736 - val_accuracy: 0.5103\n",
      "\n",
      "Epoch 1/10                                                                            \n",
      "192/192 - 87s - loss: 0.9881 - accuracy: 0.4988 - val_loss: 0.9804 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 2/10                                                                            \n",
      "192/192 - 86s - loss: 0.9858 - accuracy: 0.4979 - val_loss: 0.9788 - val_accuracy: 0.5007\n",
      "\n",
      "Epoch 3/10                                                                            \n",
      "192/192 - 86s - loss: 0.9861 - accuracy: 0.5009 - val_loss: 0.9836 - val_accuracy: 0.4966\n",
      "\n",
      "Epoch 4/10                                                                            \n",
      "192/192 - 86s - loss: 0.9843 - accuracy: 0.4980 - val_loss: 0.9861 - val_accuracy: 0.5025\n",
      "\n",
      "Epoch 5/10                                                                            \n",
      "192/192 - 87s - loss: 0.9825 - accuracy: 0.4978 - val_loss: 0.9730 - val_accuracy: 0.4982\n",
      "\n",
      "Epoch 6/10                                                                            \n",
      "192/192 - 86s - loss: 0.9844 - accuracy: 0.4980 - val_loss: 0.9782 - val_accuracy: 0.4998\n",
      "\n",
      "Epoch 7/10                                                                            \n",
      "192/192 - 86s - loss: 0.9844 - accuracy: 0.5025 - val_loss: 0.9807 - val_accuracy: 0.4985\n",
      "\n",
      "Epoch 8/10                                                                            \n",
      "192/192 - 86s - loss: 0.9831 - accuracy: 0.5021 - val_loss: 0.9706 - val_accuracy: 0.4938\n",
      "\n",
      "Epoch 9/10                                                                            \n",
      "192/192 - 86s - loss: 0.9827 - accuracy: 0.4967 - val_loss: 0.9687 - val_accuracy: 0.5013\n",
      "\n",
      "Epoch 10/10                                                                           \n",
      "192/192 - 86s - loss: 0.9828 - accuracy: 0.5004 - val_loss: 0.9677 - val_accuracy: 0.4979\n",
      "\n",
      "Epoch 1/20                                                                            \n",
      "383/383 - 743s - loss: 0.4699 - accuracy: 0.5050 - val_loss: 0.3643 - val_accuracy: 0.5088\n",
      "\n",
      "Epoch 2/20                                                                          \n",
      "383/383 - 749s - loss: 0.3756 - accuracy: 0.5049 - val_loss: 0.3648 - val_accuracy: 0.4851\n",
      "\n",
      "Epoch 3/20                                                                          \n",
      "383/383 - 750s - loss: 0.3730 - accuracy: 0.5045 - val_loss: 0.3721 - val_accuracy: 0.5052\n",
      "\n",
      "Epoch 4/20                                                                          \n",
      "383/383 - 756s - loss: 0.3765 - accuracy: 0.5076 - val_loss: 0.3682 - val_accuracy: 0.5059\n",
      "\n",
      "Epoch 5/20                                                                          \n",
      "383/383 - 756s - loss: 0.3759 - accuracy: 0.5018 - val_loss: 0.3734 - val_accuracy: 0.4959\n",
      "\n",
      "Epoch 6/20                                                                          \n",
      "383/383 - 755s - loss: 0.3706 - accuracy: 0.4940 - val_loss: 0.3865 - val_accuracy: 0.5196\n",
      "\n",
      "Epoch 7/20                                                                          \n",
      "383/383 - 758s - loss: 0.3774 - accuracy: 0.5040 - val_loss: 0.3789 - val_accuracy: 0.5109\n",
      "\n",
      "Epoch 8/20                                                                          \n",
      "383/383 - 755s - loss: 0.3713 - accuracy: 0.4949 - val_loss: 0.3879 - val_accuracy: 0.5093\n",
      "\n",
      "Epoch 9/20                                                                          \n",
      "383/383 - 757s - loss: 0.3765 - accuracy: 0.4975 - val_loss: 0.3968 - val_accuracy: 0.5008\n",
      "\n",
      "Epoch 10/20                                                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 - 780s - loss: 0.3811 - accuracy: 0.5025 - val_loss: 0.3734 - val_accuracy: 0.4958\n",
      "\n",
      "Epoch 11/20                                                                         \n",
      "383/383 - 773s - loss: 0.3792 - accuracy: 0.5030 - val_loss: 0.3985 - val_accuracy: 0.5082\n",
      "\n",
      "Epoch 12/20                                                                         \n",
      "383/383 - 766s - loss: 0.3815 - accuracy: 0.5082 - val_loss: 0.3796 - val_accuracy: 0.4992\n",
      "\n",
      "Epoch 13/20                                                                         \n",
      " 60%|██████    | 3/5 [3:46:59<57:45, 1732.68s/trial, best loss: -0.5607843399047852]"
     ]
    }
   ],
   "source": [
    "def try_hyperas():\n",
    "    try:\n",
    "        best_run, best_model, space = optim.minimize(model=create_model,\n",
    "                                                     data=data,\n",
    "                                                     algo=tpe.suggest,\n",
    "                                                     max_evals=5,\n",
    "                                                     trials=Trials(),\n",
    "                                                     eval_space=True,\n",
    "                                                     notebook_name='hyperas_optimization',\n",
    "                                                     return_space=True)\n",
    "        \n",
    "        return best_run, best_model, space\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e.message)\n",
    "        print(e)\n",
    "        \n",
    "        return None, None, None \n",
    "        \n",
    "best_run, best_model, space = try_hyperas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65563786",
   "metadata": {},
   "source": [
    "### Melhores resultados\n",
    "Resultado dos melhores hiperparâmetros encontrados assim como o evaluate dos valores de loss e métricas do modelo treinado.\n",
    "\n",
    "Como é apresentado no [link](https://stackoverflow.com/questions/44843581/what-is-the-difference-between-model-fit-an-model-evaluate-in-keras) diferença entre:\n",
    "\n",
    "- *fit()*: is for training the model with the given inputs (and corresponding training labels);\n",
    "\n",
    "- *evaluate()*: is for evaluating the already trained model using the validation (or test) data and the corresponding labels. Returns the loss value and metrics values for the model;\n",
    "\n",
    "- *predict()*: is for the actual prediction. It generates output predictions for the input samples.\n",
    "\n",
    "Logo o hyperas realiza a avaliação do modelo treinado e não a predição de dados dado como entrada e com resultados de saída, o qual no caso do meu trabalho seriam valores entre 0 e 1, onde quanto mais próximo de 1 mais similar são duas instâncias de textos indicando que possivelmente são de mesmo autores. Caso contrário são assimilares e de autores distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f7365",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_run == None:\n",
    "    print(\"It was not possible to perform your best model hyperparameters!\")\n",
    "else:\n",
    "    x_train, y_train, x_test, y_test = data()\n",
    "    \n",
    "    print(\"Evalutation of best performing model:\")\n",
    "    print(best_model.evaluate([x_test['left'], x_test['right']], y_test))\n",
    "    \n",
    "    print(\"Best performing model chosen hyperparameters:\")\n",
    "    print(best_run)\n",
    "    \n",
    "    # realizar aqui o código que guarda em um arquivo .CSV os melhores hyperparâmetros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
